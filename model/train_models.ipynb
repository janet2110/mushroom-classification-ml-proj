{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom Classification - Machine Learning Assignment 2\n",
    "\n",
    "**Student Details:**\n",
    "- **BITS ID:** 2025AA05835\n",
    "- **Name:** JANET DEVARAJ\n",
    "- **Email:** 2025aa05835@wilp.bits-pilani.ac.in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, precision_score, \n",
    "                             recall_score, f1_score, matthews_corrcoef,\n",
    "                             confusion_matrix, classification_report)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from UCI repository\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\"\n",
    "columns = ['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "           'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
    "           'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "           'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
    "           'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n",
    "           'ring-type', 'spore-print-color', 'population', 'habitat']\n",
    "\n",
    "df = pd.read_csv(url, names=columns)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(f\"Missing values before cleaning: {df.isnull().sum().sum()}\")\n",
    "df = df.replace('?', np.nan)\n",
    "print(f\"Missing values (marked as ?): {df.isnull().sum().sum()}\")\n",
    "df = df.dropna()\n",
    "print(f\"Shape after removing missing values: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for all categorical features\n",
    "label_encoders = {}\n",
    "for column in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "print(\"✅ All features encoded successfully!\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nNumber of features: {X.shape[1]}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTesting set class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"Calculate all required evaluation metrics\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "        'f1': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'mcc': matthews_corrcoef(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    if y_proba is not None:\n",
    "        try:\n",
    "            if len(np.unique(y_true)) == 2:\n",
    "                metrics['auc'] = roc_auc_score(y_true, y_proba[:, 1])\n",
    "            else:\n",
    "                metrics['auc'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='weighted')\n",
    "        except:\n",
    "            metrics['auc'] = 0.0\n",
    "    else:\n",
    "        metrics['auc'] = 0.0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "lr_proba = lr_model.predict_proba(X_test)\n",
    "lr_metrics = evaluate_model(y_test, lr_pred, lr_proba)\n",
    "\n",
    "print(\"\\nLogistic Regression Metrics:\")\n",
    "for metric, value in lr_metrics.items():\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Decision Tree...\")\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_pred = dt_model.predict(X_test)\n",
    "dt_proba = dt_model.predict_proba(X_test)\n",
    "dt_metrics = evaluate_model(y_test, dt_pred, dt_proba)\n",
    "\n",
    "print(\"\\nDecision Tree Metrics:\")\n",
    "for metric, value in dt_metrics.items():\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training K-Nearest Neighbors...\")\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "knn_proba = knn_model.predict_proba(X_test)\n",
    "knn_metrics = evaluate_model(y_test, knn_pred, knn_proba)\n",
    "\n",
    "print(\"\\nK-Nearest Neighbors Metrics:\")\n",
    "for metric, value in knn_metrics.items():\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Naive Bayes...\")\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_pred = nb_model.predict(X_test)\n",
    "nb_proba = nb_model.predict_proba(X_test)\n",
    "nb_metrics = evaluate_model(y_test, nb_pred, nb_proba)\n",
    "\n",
    "print(\"\\nNaive Bayes Metrics:\")\n",
    "for metric, value in nb_metrics.items():\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Random Forest Classifier (Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_proba = rf_model.predict_proba(X_test)\n",
    "rf_metrics = evaluate_model(y_test, rf_pred, rf_proba)\n",
    "\n",
    "print(\"\\nRandom Forest Metrics:\")\n",
    "for metric, value in rf_metrics.items():\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. XGBoost Classifier (Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost...\")\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_proba = xgb_model.predict_proba(X_test)\n",
    "xgb_metrics = evaluate_model(y_test, xgb_pred, xgb_proba)\n",
    "\n",
    "print(\"\\nXGBoost Metrics:\")\n",
    "for metric, value in xgb_metrics.items():\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "results = {\n",
    "    'Logistic Regression': lr_metrics,\n",
    "    'Decision Tree': dt_metrics,\n",
    "    'K-Nearest Neighbors': knn_metrics,\n",
    "    'Naive Bayes': nb_metrics,\n",
    "    'Random Forest': rf_metrics,\n",
    "    'XGBoost': xgb_metrics\n",
    "}\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "comparison_df.columns = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'MCC', 'AUC']\n",
    "comparison_df = comparison_df[['Accuracy', 'AUC', 'Precision', 'Recall', 'F1 Score', 'MCC']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS - All Models Comparison\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.round(4))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_list = ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1 Score', 'MCC']\n",
    "\n",
    "for idx, metric in enumerate(metrics_list):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    comparison_df[metric].plot(kind='bar', ax=ax, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(metric, fontweight='bold')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models\n",
    "trained_models = {\n",
    "    'Logistic Regression': lr_model,\n",
    "    'Decision Tree': dt_model,\n",
    "    'K-Nearest Neighbors': knn_model,\n",
    "    'Naive Bayes': nb_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'XGBoost': xgb_model\n",
    "}\n",
    "\n",
    "with open('trained_models.pkl', 'wb') as f:\n",
    "    pickle.dump(trained_models, f)\n",
    "\n",
    "with open('metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "with open('label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "# Save test data\n",
    "X_test.to_csv('test_data.csv', index=False)\n",
    "pd.DataFrame({'class': y_test}).to_csv('test_labels.csv', index=False)\n",
    "\n",
    "print(\"✅ All models, metrics, and data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "All 6 models have been successfully trained and evaluated on the mushroom classification dataset. The results show excellent performance across most models, with ensemble methods achieving perfect classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
